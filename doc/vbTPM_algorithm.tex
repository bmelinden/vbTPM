We will start by discussing the statistical analysis of the simple HMM
in detail, and then discuss generalization to the factorial model.
\subsection{Model selection by maximum evidence}
Our analysis aims not only to extract parameter values from TPM data,
but also to learn the number of hidden states $N$, corresponding to
different DNA-protein conformations. This means that we need to
compare models with different number of unknown parameters.  We take a
Bayesian approach to this problem.

A distinguishing feature of Bayesian data analysis is the treatment of
random variables and unknown parameters on an equal footing
\cite{eddy2004,mackay2003}. 
Hence, given some data $\x_{1:T}$ and a set of competing models with
different number of states $N=1,2,\ldots$ (and \mbox{\textit{1:T}} is
a compact way to denote a whole time series), we can use the laws
of probability to express our confidence about those models in terms
of conditional probabilities,
\begin{equation}
  p(N|\x_{1:T})=p(\x_{1:T}|N)p(N)/p(\x_{1:T}),
\end{equation}
where $p(N)$ expresses our beliefs about the different models prior to
seeing the data, and $p(\x_{1:T})$ is a normalization constant. A
Bayesian rule for model selection is therefore to prefer the model
that maximizes $p(\x_{1:T}|N)$, a quantity known as the evidence. For
our more complex model, parameters and hidden states will have to be
integrated out,
\begin{equation}\label{eq:evidencedef}
  p(\x_{1:T}|N)=\int d\theta\sum_{s_{1:T}} p(\x_{1:T},s_{1:T}|\theta)p(\theta|N),
\end{equation}
where the first factor in the integrand describes the model, and the
second expresses our prior beliefs about the parameters (see
below).

The integrand in the evidence, \Eq{eq:evidencedef}, requires an
explicit expression for the probability of a sequence of bead
positions and hidden states. This expression can be written down based
on the above model, and factorizes in the usual HMM fashion, as
\begin{multline}\label{eq:evidencintegrand}
 p(\x_{1:T},s_{1:T}|\theta)p(\theta|N)%\matris{A},\vec{\pi},\vec{K},\vec{B},N)
=
p(\x_1)p(s_1|\vec{\pi})\\
\times
\prod_{t=2}^{T}p(\x_t|\x_{t-1},s_t,\vec{K},\vec{B})p(s_t|s_{t-1},\matris{A})\\
\times
p(\vec{\pi}|N)
\prod_{j=1}^Np(K_j,B_j|N)p(A_{j,:}|N),
\end{multline}
where $A_{j,:}$ denote row $j$ of the matrix $\matris{A}$.  The first
right hand side line in \Eq{eq:evidencintegrand} describes the initial
state and bead position. We will neglect the factor $p(\x_1)$ from now
on, but the initial state $p(s_1|\vec\pi)$ and transition
probabilities $p(s_t|s_{t-1},\matris{A})$ are given by
\Eq{eq:eoms}, and the bead motion follows from \Eq{eq:eomx},
\begin{equation}
 p(\x_t|\x_{t-1},s_t,\vec{K},\vec{B})
 =\frac{B_{s_t}}{\pi}
 e^{-B_{s_t}(\x_t-K_{s_t}\x_{t-1})^2}.
\end{equation}
Finally, the last line of \Eq{eq:evidencintegrand} contains prior
distributions over parameters conditional on the number of states. We
use conjugate priors, parameterized to have minimal impact on the
inference results (see SI).


%The maximum evidence criterion balances model complexity versus
%goodness of fit in an optimal way, but is expensive to compute
%exactly. Instead, we use an approximation variously known as ensemble
%learning, variational Bayes, or mean field
%theory \cite{mackay2003,bishop2006}, which has previously been applied
%successfully to single molecule FRET data
%\cite{bronson2009,vandemeent_manuscript,okamoto2012} and \textit{in
%  vivo} single particle tracking \cite{persson2013}.  
%Further details are given in the SI.

%\paragraph{Parameters and hidden states}
%{\bf Explain what parameter values and hidden states we plot}. 

\subsection{The variational approximation}
An exact computation of the Bayesian evidence is impractical or
impossible for most interesting models, and clever approximations are
needed. The approximation we use here is variously known as ensemble
learning, variational Bayes, or (in statistical physics jargong) mean
field theory \cite{mackay2003,bishop2006}, has previously been applied
to biophysical time-series of FRET data
\cite{bronson2009,vandemeent_manuscript,okamoto2012} and \textit{in
  vivo} single particle tracking \cite{persson2013}.  The idea is to
approximate the log evidence by a lower bound, $\ln p(x|N)\geq F_N$,
with
\begin{equation}
  F_N= \int d\theta\sum_{s} q(s)q(\theta)
  \ln\frac{p(x,s|\theta)p(\theta|N)}{q(s)q(\theta)},
\end{equation}
where $q(s)$ and $q(\theta)$ are arbitrary probability distributions
over the hidden states and parameters respectively.  These are
optimized to make the bound as tight as possible for each model, the
model that achieves the highest lower bound wins, and the
corresponding optimal distributions $q(s)q(\theta)$ can be used for
approximate inference about parameter values and hidden states. In
particular, optimizing $F_N$ with respect to the variational
distributions leads to
\begin{align}
  \ln q(\theta)=&-\ln Z_\theta+ \ln p(\theta|N)
  +\mean{\ln p(x,s|\theta)}_{q(s)},\label{seq:qparam}\\
  \ln q(s)=&-\ln Z_s+\mean{\ln p(x,s|\theta)}_{q(\theta)},\label{seq:sparam}
\end{align}
where the $Z$'s are Lagrange multipliers to enforce normalization, and
$\mean{\cdot}_{q(\cdot)}$ denotes an average over $q(\cdot)$. We solve
these equations iteratively until the lower bound converges, repeating
the analysis many times with independent initial conditions in order
to find a global maximum.  The iterative solution approach results in
an EM-type variational algorithm, detailed below.  We refer to
Refs.~\cite{mackay1997,beal2003,persson2013} for details on how to
derive variational algorithms for HMMs, and
Refs.~\cite{beal2003,mackay2003,bishop2006} for more general
discussion of variational inference methods.

\subsubsection{Parameter distributions}
The results of plugging our diffusinve HMM into the parameter update
equation \eqref{seq:qparam} are as follows.  The initial state
probability vector, and each row in the transition matrix (denoted
$A_{j,:}$), are Dirichlet distributed,
\begin{align}
q(\vec{\pi})=&\Dir(\vec{\pi}|\vec{w}^{(\vec{\pi})}),\label{seq:VBpi}\\
w_j^{(\vec{\pi})}=&\tilde w_j^{(\vec{\pi})}+\mean{\delta_{j,s_1}}_{q(s_{1:T})},
\label{seq:VBMpi}\\
%\end{align}\begin{align}
q(A_{i,:})=&\Dir(A_{i,:}|\vec{w}^{(\matris{A})}),\\
w_{ij}^{(\matris{A})}=&\tilde w_{ij}^{(\matris{A})}
+\sum_{t=2}^T\mean{\delta_{i,s_{t-1}}\delta_{j,s_t}}_{q(s_{1:T})}.\label{seq:VBMAj}
\end{align}
Here, variables under tilde's ($\tilde{~}$) are hyperparameters that
parameterize the prior distributions, and can be interpreted as
pseudo-observations. The Dirichlet density function is
\begin{equation}
  \Dir(\vec \pi | \vec u)=\frac{\Gamma(u_0)}{\prod_j\Gamma(u_j)}
  \prod_j \pi_j^{u_j-1},\quad u_j>1,
\end{equation}
where $u_0=\sum_j u_j$ is called the strength, and the density is
non-zero in the region $0\le\pi_j\le1$, $\sum_j\pi_j=1$ . Before
moving on, we quote some useful expectation values for future
reference,
\begin{align}
  \mean{\ln\pi_i}_{q(\vec{\pi})}=&\psi(w_i^{(\vec{\pi})})-\psi(w_0^{(\vec{\pi})}),
  \label{seq:mlogpi}\\
  \mean{\ln A_{ij}}_{q(\matris{A})}=&\psi(w_{ij}^{(\matris{A})})
  -\psi(w_{i0}^{(\matris{A})}),\label{seq:mlogA}
\end{align}
where $\psi(x)$ is the digamma function, and
\begin{align}
  \mean{\pi_i}_{q(\vec{\pi})}=&\frac{w_i^{(\vec{\pi})}}{w_0^{(\vec{\pi})}},\\
  \Var[\pi_i]_{q(\vec{\pi})}=&
  \frac{w_i^{(\vec{\pi})}\big((1-w_i^{(\vec{\pi})}\big)}{
    (w_0^{(\vec{\pi})})^2\big(1+w_0^{(\vec{\pi})}\big)},\\
  \mean{A_{ij}}_{q(\matris{A})}=&\frac{w_{ij}^{(\matris{A})}}{w_{i0}^{(\matris{A})}},\\
  \Var[A_{ij}]_{q(\matris{A})}=&
  \frac{w_{ij}^{(\matris{A})}\big(1-w_{ij}^{(\matris{A})}\big)}{
    (w_{i0}^{(\matris{A})})^2\big(1+w_{i0}^{(\matris{A})}\big)}.
\end{align}
The bead motion parameters have the following variational distributions
\begin{align}\label{seq:KBtrial}
  q(K_j,B_j)=&\frac{B_j^{n_j}}{W_j}e^{-B_j\big(v_j(K_j-\mu_j)^2+c_j\big)},\\
  W_j=&\frac{c^{-(n_j+\frac 12)}\Gamma(n_j+\frac 12)}{\sqrt{v_j/\pi}},
\end{align}
with the range $B_j\ge 0$, $-\infty<K_j<\infty$. Physically, we might
rather expect $0<K_j<1$, but the extended range for $K_j$ simplifies
the calculations a lot. The VBM equations are
\begin{align}
  n_j=& \tilde n_j+M_j,\label{seq:nj}\\
  c_j=& \tilde c_j+C_j+\tilde v_j\tilde\mu_j^2
  -\frac{\big(\tilde v_j\tilde\mu_j+U_j\big)^2}{\tilde v_j+V_j},\label{seq:cj}\\
  v_j=&\tilde v_j+V_j,\label{seq:vj}\\
  \mu_j=&\frac{\tilde v_j\tilde\mu_j+U_j}{\tilde v_j+V_j},\label{seq:muj}\\
\end{align}
where hyperparameters describing the prior distribution are again
indicated by $\tilde{\phantom{n}}$, and the (conjugate) prior
distributions are recovered by setting $M_j=C_j=U_j=V_j=0$. These
data-dependent terms are given by
\begin{align}
  M_j=&\sum_{t=2}^T\mean{\delta_{s_t,j}},\label{seq:Mj}\\
  C_j=&\sum_{t=2}^T\mean{\delta_{s_t,j}}\x_t^2,\label{seq:Cj}\\
  V_j=&\sum_{t=2}^T\mean{\delta_{s_t,j}}\x_{t-1}^2.\label{seq:Vj}\\
  U_j=&\sum_{t=2}^T\mean{\delta_{s_t,j}}\x_t\cdot\x_{t-1}.\label{seq:Uj}
\end{align}

Some useful expectation values for future reference are
\begin{align}
  \mean{K_j}_{q(\vec{B},\vec{K})}=&\mu_j,  \label{seq:meanK}\\
  \text{Var}[K_j]_{q(\vec{B},\vec{K})}=&\frac{c_j}{2v_j(n_j-\frac 12)}.
  \label{seq:varK}\\
  \mean{B_j}_{q(\vec{B},\vec{K})}=&\frac{n_j+\frac 12}{c_j},  \label{seq:meanB}\\
  \text{Var}[B_j]_{q(\vec{B},\vec{K})}=&\frac{n_j+\frac 12}{c_j^2},  
  \label{seq:varB}\\
  \mean{\ln B_j}_{q(\vec{B},\vec{K})}=&\psi\big(n_j+\frac 12\big)-\ln c_j,
  \label{seq:meanlogB}\\
  \mean{B_jK_j^2}_{q(\vec{B},\vec{K})}=&\frac{1}{2v_j}
  +\mu_j^2\frac{n_j+\frac 12}{c_j},  \label{seq:meanBK2}\\
  \mean{B_jK_j}_{q(\vec{B},\vec{K})}=&\mu_j\frac{n_j+\frac 12}{c_j},
  \label{seq:meanBK}
\end{align}

In addition to being needed during the algorithm, these averages can
be used to translate prior knowledge in terms of means and standard
deviations of $K_j,B_j$ into prior parameters $\tilde n_j, \tilde c_j,
\tilde \mu_j, \tilde v_j$

\subsubsection{Hidden state distribution}
The variational distribution has a simple form,
\begin{equation}\label{seq:qgt}
  \ln q(s_{1:T})=-\ln Z+\sum_{t=1}^T\ln h_{s_t}(t)+\sum_{t=2}^T\ln J_{s_{t-1},s_t},
\end{equation}
i.e., an initial state distribution, a point-wise term that depends on
the initial conditions and the data, and a transition
probability. The point-wise 
\begin{equation}
  \ln q(s_{1:T})=-\ln Z+\sum_{t=1}^T\ln h_{s_t}(t)+\sum_{t=2}^T\ln J_{s_{t-1},s_t},
\end{equation}
i.e., an initial state distribution, point-wise terms that depends on
the initial conditions and the data, and transition terms. The
mathematical form of this expression is the same as encountered in
maximum-likelihood optimization of hidden Markov Models, and hence the
normalization constant and expectation values needed for the parameter
update equations can be computed by the Baum-Welch algorithm
\cite{baum1970}, which resembles the transfer matrix solution for spin
models in statistical physics. 

Similarly, and the most likely sequence of hidden states can be
computed by the Viterbi algorithm \cite{viterbi1967}.

Specifically, the initial term is given by
\begin{equation}
  \ln h_j(1)=
  \mean{p(s_1=j|\vec{\pi})}_{q(\vec{\pi})}
  =\psi(w_{j}^{(\vec{\pi})})-\psi(w_0^{(\vec{\pi})}),
\end{equation}
the point-wise contributions for $t>1$ are
\begin{multline}
  \ln h_j(t)=
  \psi\big(n_j+\frac12\big)-\ln(\pi c_j)-\frac{\x_{t-1}^2}{2v_j}\\
-\frac{n_j+\frac12}{c_j}\Bigg(
\x_{t-1}^2\bigg(\mu_j-\frac{\x_t\cdot\x_{t-1}}{\x_{t-1}^2}\bigg)^2\\
+\x_t^2-\frac{\big(\x_t\cdot\x_{t-1}\big)^2}{\x_{t-1}^2}\Bigg),
\end{multline}
and the transition terms are given by
\begin{equation}
    \ln J_{ji}=\psi\big(w_{j,i}^{(\matris{A})}\big)
    -\psi\big(\sum_{k=1}^Nw_{j,k}^{(\matris{A})}\big).
\end{equation}
\subsection{VBEM iterations and model search}
The iterative optimization of the variational distributions are done
as follows. To start with, an initial guess for the variational
parameter distributions are generated. We then alternate between VBE
step, in which we construct the hidden state distribution and compute
the averages $\mean{\delta_{j,s_t}}_{q(s)}$ and
$\mean{\delta_{j,s_t}\delta_{k,s_{t+1}}}_{q(s)}$ in a Baum-Welch
forward-backward sweep, and a VBM step, in which we use these averages
to update the parameter variational distributions, until the lower
bound converges.

The variational approach has the additional useful tendency to
penalizing overfitting already during the VBEM iterations, by
depopulating superfluous states
\cite{mackay1997,beal2003,persson2013}. We exploit this property by
using a greedy search algorithm to explore the model space. The basic
strategy is to start by fitting a model with many states from random
initial conditions, and then exploring less complex models by
gradually removing the least populated states. This saves computing
time by supplying good initial guesses for the low complexity models
(which therefore converge quickly), and by lowering the number of
independent restarts, since it is easier to construct a good initial
guess for a model with many states.

\subsection{The lower bound}
has an especially simple form just after the VBE step
\cite{mackay1997,beal2003,persson2013}, given by the normalization
constant $\ln Z$ of the variational hidden state distribution, minus
the Kullback-Leibler divergences between the variational and prior
parameter distributions,
\begin{multline}
  F=\ln Z
  -\int d\vec{\pi} q(\vec{\pi}) \ln\frac{q(\vec{\pi})}{p(\vec{\pi})}\\
  -\sum_{j=1}^N\Bigg[
  \int d^NA_{j,:}\;q(A_{j,:})\ln\frac{q(A_{j,:})}{p_0(A_{j,:})}\\
  +\int dB_jdK_j\;q(B_j,K_j)\ln\frac{q(B_j,K_j)}{p_0(B_j,K_j)}
  \Bigg].
\end{multline}
The Kullback-Leibler terms can be expressed in terms of the
expectation values computed above. For the initial state distribution,
we get
\begin{multline}
  \int d\vec{\pi} q(\vec{\pi}) \ln\frac{q(\vec{\pi})}{p_0(\vec{\pi})}
  =\ln\tilde w_0^{(\vec{\pi})}
    -\psi(\tilde w_0^{(\vec{\pi})})-\frac{1}{\tilde w_0^{(\vec{\pi})}}\\
    +\sum_{j=1}^N\left[
      \big(w_j^{(\vec{\pi})}-\tilde w_j^{(\vec{\pi})}\big)\psi(w_j^{(\vec{\pi})})
      -\ln\frac{\Gamma(w_j^{(\vec{\pi})})}{\Gamma(\tilde w_j^{(\vec{\pi})})}
      \right].
\end{multline}
To get this simple form, we used that $w_0^{(\vec{\pi})}=1+\tilde
w_0^{(\vec{\pi})}$ (since $\sum_j\mean{\delta_{j,s_1}}=1$), and the
identities $\Gamma(x+1)=x\Gamma(x)$ and $\psi(x+1)=\psi(x)+\frac 1x$.
Furthermore, each row of the transition probability matrix contributes
\begin{multline}
  \int d^NA_{j,:}\;q(A_{j,:})\ln\frac{q(A_{j,:})}{p_0(A_{j,:})}\\
  =\ln\frac{\Gamma(w_{j0}^{(\matris{A})})}{\Gamma(\tilde w_{j0}^{(\matris{A})})}
    -(w_{j0}^{(\matris{A})}-\tilde w_{j0}^{(\matris{A})})\psi(w_{j0}^{(\matris{A})})\\
    -\sum_{k=1}^N\bigg[
      \ln\frac{\Gamma(w_{jk}^{(\matris{A})})}{\Gamma(\tilde w_{jk}^{(\matris{A})})}
      -\big(w_{jk}^{(\matris{A})}-\tilde w_{jk}^{(\matris{A})}\big)
      \psi(w_{jk}^{(\matris{A})})\bigg].
\end{multline}
Finally, the emission parameter of each state contributes
\begin{multline}\label{seq:KBKL}
  \int dB_j\int d^NK_j\;q(B_j,K_j)
  \ln\frac{q(B_j,K_j)}{p(B_j,K_j)}=\ldots\\
  =-\frac{n_j+\frac 12}{c_j}
    \Big(c_j-\tilde c_j-\tilde v_j(\mu_j-\tilde \mu_j)^2\Big)\\
    +\frac 12\ln\frac{v_j}{\tilde v_j}
    +(\tilde n_j+\frac 12)\ln\frac{c_j}{\tilde c_j}
    -\ln\frac{\Gamma\big(n_j+\frac 12\big)}{\Gamma\big(\tilde n_j+\frac 12\big)}\\
    +(n_j-\tilde n_j)\psi\big(n_j+\frac 12\big)
    +\frac{\tilde v_j}{2v_j}
    -\frac 12.
\end{multline}

\subsection{Specification of prior distributions}
\paragraph{Emission parameters}
We specify priors for the emission parameters $K,B$ in terms of the
the mean values $\mean{K_j}$, $\mean{B_j}$, the standard deviation of
$K_j$, and the number of pseudo-counts $\tilde n_j$, and then solve
for the remaining hyper-parameters $\tilde c_j, \tilde \mu_j,\tilde
v_j$ using Eqs.~(\ref{seq:meanK}-\ref{seq:varB}).  We take the same
hyperparameters for all states, and independent of the number of
states $N$. Priors for spurious states are specified analogously.

\paragraph{Initial state}
The initial state probability vector has a Dirichlet prior with
weights $\tilde\vec w^{(\vec{\pi})}$ (see \Eq{seq:VBMpi}). We choose a
constant total strength $f^{(\vec\pi)}$, i.e.,
\begin{equation}\label{seq:fPi}
  \tilde w_j^{(\vec{\pi})}=f^{(\vec\pi)}/N.
\end{equation}

\paragraph{Transition matrix}
The prior for the transition matrix is independent Dirichlet
distributions for each row (see \Eq{seq:VBMAj}), with pseudo-count
matrix $\tilde w_{ij}^{(\matris{A})}$. Following \citet{persson2013},
we parameterize this prior in terms of an expected mean dwell time and
an overall number of pseudocounts (prior strength) for each hidden
state. To make the definition invariant under changes of sampling
time, we specify the strength $t_A$ and prior mean dwell time $t_D$ in
time units. We then construct a transition \textit{rate} matrix Q with
mean dwell time $t_D$,
\begin{equation}\label{seq:Qprior}
  Q_{ij}=\frac{1}{t_D}\left(-\delta_{ij}
  +\frac{1-\delta_{ij}}{N-1}\right),
\end{equation}
and construct the pseudo-counts based on the transition probability
propagator per unit time step,
\begin{equation}\label{seq:wAprior}
  \tilde w_{ij}^{(\matris{A})}=\frac{t_Af_{sample}}{n_{downsample}}
  e^{\Delta tQ}.
\end{equation}
This expression uses the downsampled timestep $\Delta
t=n_{downsample}/f_{sample}$, where $f_{sample}$ is the sampling
frequency (30 Hz in our case), and $n_{downsample}$ is the
downsampling factor.  Numerical experiments by \citet{persson2013}
show that choosing the strength too low compared to the mean dwell
time produces a bias towards sparse transition matrices.  This is
often not desireable, and we use $t_D=1$ s and $t_A=5$ s as default
parameters, and also throughout our work \cite{vbTPMpaper}.

Details on how top specify parameters for prior distributions are
given in table \ref{stab:priors}.

\begin{table}
  \begin{center}
    \begin{tabular}{|c|l|c|}
      \hline
      variable & comment & Eq.\\    
      \hline
      \hline
      \verb+B0+ & $\mean{B_j}$ & \eqref{seq:meanB}\\
      \verb+fB+ & $\tilde n_j$ &   \eqref{seq:nj}\\
      \verb+K0+ & $\tilde\mu_j$ &  \eqref{seq:muj}\\
      \verb+Kstd+& $\sqrt{\Var{K_j}}$ & \eqref{seq:varK}\\
      \verb+KBscaling+& 
      \begin{minipage}[t]{0.5\columnwidth}
        Possibility to specify an obsolete state-dependence of the prior
        parameters for $K_j$, $B_j$. We recommend the default value 2.
      \end{minipage}&\\
      \hline
      &\begin{minipage}[t]{0.5\columnwidth}
         \verb+Bc0+, \verb+fBc+, \verb+Kc0+, and \verb+KcStd+ are
         spurious state parameters, with the same interpretation as for
         genuine states. 
       \end{minipage}&\\
      \hline
      \verb+fPi+& $f^{(\vec\pi)}$ & \eqref{seq:fPi}\\
      \verb+tD+ & \begin{minipage}[t]{0.5\columnwidth}
        Mean prior dwell time $t_D$. Default 1 s.  \end{minipage}& \eqref{seq:Qprior}\\
      \verb+tA+ & \begin{minipage}[t]{0.5\columnwidth}
        $\matris{A}$ prior strength $t_A$. Default 5 s. \end{minipage}& \eqref{seq:wAprior}\\
      \hline
      \verb+tDc0+ & 
      \begin{minipage}[t]{0.5\columnwidth}
        Mean prior dwell time of the genuine indicator state
        $c_t=1$. Default 10 s. \end{minipage}&\\
      \hline
    \end{tabular}
  \end{center}
  \caption{Specifying prior parameters in runinput files. See also the
    documentation of VB7\_priorParent, and the example runinput
    files.}\label{stab:priors}  
\end{table}
\subsection{Model search}
To find the best number of states, we adopt a greedy search approach
with multiple restarts, similar to the one used in vbSPT
\cite{persson2013}. The basic idea is to exploit the fact that the
variational Bayes EM iterations tend to sense overfitting by making
superfluous states unpopulated. Each search sweep thus starts by
convrging the largest model to be considered, and then gradually
removes low-occupancy states until the lower bound $F$ does not
improve any more. We also attempt to reconverge each size with some
extra pseudo-counts added to the transition matrix to avoid getting
stuck in non-interconverting models.

Inital conditions for the largest model are generated randomly, partly
based on user-specified intervals for the $K$ and $B$ parameters, and
partly using the actual data to find high-occupancy values of
$K,B$. For the transition matrices, we initialize using a mean dwell
time, similar as for the prior distributions, but with higher total
strength. Details are given in the documentation of
\verb+VB7_initialGuess_KBregion.m+, and in the example runinput file.
