We will start by discussing the statistical analysis of the simple HMM
in detail, and then discuss generalization to the factorial model.
\subsection{Model selection by maximum evidence}
Our analysis aims not only to extract parameter values from TPM data,
but also to learn the number of hidden states $N$, corresponding to
different DNA-protein conformations. This means that we need to
compare models with different number of unknown parameters.  We take a
Bayesian approach to this problem.

A distinguishing feature of Bayesian data analysis is the treatment of
random variables and unknown parameters on an equal footing
\cite{eddy2004,mackay2003}. 
Hence, given some data $\x_{1:T}$ and a set of competing models with
different number of states $N=1,2,\ldots$ (and \mbox{\textit{1:T}} is
a compact way to denote a whole time series), we can use the laws
of probability to express our confidence about those models in terms
of conditional probabilities,
\begin{equation}
  p(N|\x_{1:T})=p(\x_{1:T}|N)p(N)/p(\x_{1:T}),
\end{equation}
where $p(N)$ expresses our beliefs about the different models prior to
seeing the data, and $p(\x_{1:T})$ is a normalization constant. A
Bayesian rule for model selection is therefore to prefer the model
that maximizes $p(\x_{1:T}|N)$, a quantity known as the evidence. For
our more complex model, parameters and hidden states will have to be
integrated out,
\begin{equation}\label{eq:evidencedef}
  p(\x_{1:T}|N)=\int d\theta\sum_{s_{1:T}} p(\x_{1:T},s_{1:T}|\theta)p(\theta|N),
\end{equation}
where the first factor in the integrand describes the model, and the
second expresses our prior beliefs about the parameters (see
below).

The integrand in the evidence, \Eq{eq:evidencedef}, requires an
explicit expression for the probability of a sequence of bead
positions and hidden states. This expression can be written down based
on the above model, and factorizes in the usual HMM fashion, as
\begin{multline}\label{eq:evidencintegrand}
 p(\x_{1:T},s_{1:T}|\theta)p(\theta|N)%\matris{A},\vec{\pi},\vec{K},\vec{B},N)
=
p(\x_1)p(s_1|\vec{\pi})\\
\times
\prod_{t=2}^{T}p(\x_t|\x_{t-1},s_t,\vec{K},\vec{B})p(s_t|s_{t-1},\matris{A})\\
\times
p(\vec{\pi}|N)
\prod_{j=1}^Np(K_j,B_j|N)p(A_{j,:}|N),
\end{multline}
where $A_{j,:}$ denote row $j$ of the matrix $\matris{A}$.  The first
right hand side line in \Eq{eq:evidencintegrand} describes the initial
state and bead position. We will neglect the factor $p(\x_1)$ from now
on, but the initial state $p(s_1|\vec\pi)$ and transition
probabilities $p(s_t|s_{t-1},\matris{A})$ are given by
\Eq{eq:eoms}, and the bead motion follows from \Eq{eq:eomx},
\begin{equation}
 p(\x_t|\x_{t-1},s_t,\vec{K},\vec{B})
 =\frac{B_{s_t}}{\pi}
 e^{-B_{s_t}(\x_t-K_{s_t}\x_{t-1})^2}.
\end{equation}
Finally, the last line of \Eq{eq:evidencintegrand} contains prior
distributions over parameters conditional on the number of states. We
use conjugate priors, parameterized to have minimal impact on the
inference results (see SI).


%The maximum evidence criterion balances model complexity versus
%goodness of fit in an optimal way, but is expensive to compute
%exactly. Instead, we use an approximation variously known as ensemble
%learning, variational Bayes, or mean field
%theory \cite{mackay2003,bishop2006}, which has previously been applied
%successfully to single molecule FRET data
%\cite{bronson2009,vandemeent_manuscript,okamoto2012} and \textit{in
%  vivo} single particle tracking \cite{persson2013}.  
%Further details are given in the SI.

%\paragraph{Parameters and hidden states}
%{\bf Explain what parameter values and hidden states we plot}. 

\subsection{The variational approximation}
An exact computation of the Bayesian evidence is impractical or
impossible for most interesting models, and clever approximations are
needed. The approximation we use here is variously known as ensemble
learning, variational Bayes, or (in statistical physics jargong) mean
field theory \cite{mackay2003,bishop2006}, has previously been applied
to biophysical time-series of FRET data
\cite{bronson2009,vandemeent_manuscript,okamoto2012} and \textit{in
  vivo} single particle tracking \cite{persson2013}.  The idea is to
approximate the log evidence by a lower bound, $\ln p(x|N)\geq F_N$,
with
\begin{equation}
  F_N= \int d\theta\sum_{s} q(s)q(\theta)
  \ln\frac{p(x,s|\theta)p(\theta|N)}{q(s)q(\theta)},
\end{equation}
where $q(s)$ and $q(\theta)$ are arbitrary probability distributions
over the hidden states and parameters respectively.  These are
optimized to make the bound as tight as possible for each model, the
model that achieves the highest lower bound wins, and the
corresponding optimal distributions $q(s)q(\theta)$ can be used for
approximate inference about parameter values and hidden states. In
particular, optimizing $F_N$ with respect to the variational
distributions leads to
\begin{align}
  \ln q(\theta)=&-\ln Z_\theta+ \ln p(\theta|N)
  +\mean{\ln p(x,s|\theta)}_{q(s)},\label{seq:qparam}\\
  \ln q(s)=&-\ln Z_s+\mean{\ln p(x,s|\theta)}_{q(\theta)},\label{seq:sparam}
\end{align}
where the $Z$'s are Lagrange multipliers to enforce normalization, and
$\mean{\cdot}_{q(\cdot)}$ denotes an average over $q(\cdot)$. We solve
these equations iteratively until the lower bound converges, repeating
the analysis many times with independent initial conditions in order
to find a global maximum.  The iterative solution approach results in
an EM-type variational algorithm, detailed below.  We refer to
Refs.~\cite{mackay1997,beal2003,persson2013} for details on how to
derive variational algorithms for HMMs, and
Refs.~\cite{beal2003,mackay2003,bishop2006} for more general
discussion of variational inference methods.

\subsubsection{Parameter distributions}
The results of plugging our diffusinve HMM into the parameter update
equation \eqref{seq:qparam} are as follows.  The initial state
probability vector, and each row in the transition matrix (denoted
$A_{j,:}$), are Dirichlet distributed,
\begin{align}
q(\vec{\pi})=&\Dir(\vec{\pi}|\vec{w}^{(\vec{\pi})}),\label{seq:VBpi}\\
w_j^{(\vec{\pi})}=&\tilde w_j^{(\vec{\pi})}+\mean{\delta_{j,s_1}}_{q(s_{1:T})},
\label{seq:VBMpi}\\
%\end{align}\begin{align}
q(A_{i,:})=&\Dir(A_{i,:}|\vec{w}^{(\matris{A})}),\\
w_{ij}^{(\matris{A})}=&\tilde w_{ij}^{(\matris{A})}
+\sum_{t=2}^T\mean{\delta_{i,s_{t-1}}\delta_{j,s_t}}_{q(s_{1:T})}.\label{seq:VBMAj}
\end{align}
Here, variables under tilde's ($\tilde{~}$) are hyperparameters that
parameterize the prior distributions, and can be interpreted as
pseudo-observations. The Dirichlet density function is
\begin{equation}
  \Dir(\vec \pi | \vec u)=\frac{\Gamma(u_0)}{\prod_j\Gamma(u_j)}
  \prod_j \pi_j^{u_j-1},\quad u_j>1,
\end{equation}
where $u_0=\sum_j u_j$ is called the strength, and the density is
non-zero in the region $0\le\pi_j\le1$, $\sum_j\pi_j=1$ . Before
moving on, we quote some useful expectation values for future
reference,
\begin{align}
  \mean{\ln\pi_i}_{q(\vec{\pi})}=&\psi(w_i^{(\vec{\pi})})-\psi(w_0^{(\vec{\pi})}),
  \label{seq:mlogpi}\\
  \mean{\ln A_{ij}}_{q(\matris{A})}=&\psi(w_{ij}^{(\matris{A})})
  -\psi(w_{i0}^{(\matris{A})}),\label{seq:mlogA}
\end{align}
where $\psi(x)$ is the digamma function, and
\begin{align}
  \mean{\pi_i}_{q(\vec{\pi})}=&\frac{w_i^{(\vec{\pi})}}{w_0^{(\vec{\pi})}},\\
  \Var[\pi_i]_{q(\vec{\pi})}=&
  \frac{w_i^{(\vec{\pi})}\big((1-w_i^{(\vec{\pi})}\big)}{
    (w_0^{(\vec{\pi})})^2\big(1+w_0^{(\vec{\pi})}\big)},\\
  \mean{A_{ij}}_{q(\matris{A})}=&\frac{w_{ij}^{(\matris{A})}}{w_{i0}^{(\matris{A})}},\\
  \Var[A_{ij}]_{q(\matris{A})}=&
  \frac{w_{ij}^{(\matris{A})}\big(1-w_{ij}^{(\matris{A})}\big)}{
    (w_{i0}^{(\matris{A})})^2\big(1+w_{i0}^{(\matris{A})}\big)}.
\end{align}
The bead motion parameters have the following variational distributions
\begin{align}\label{seq:KBtrial}
  q(K_j,B_j)=&\frac{B_j^{n_j}}{W_j}e^{-B_j\big(v_j(K_j-\mu_j)^2+c_j\big)},\\
  W_j=&\frac{c^{-(n_j+\frac 12)}\Gamma(n_j+\frac 12)}{\sqrt{v_j/\pi}},
\end{align}
with the range $B_j\ge 0$, $-\infty<K_j<\infty$. Physically, we might
rather expect $0<K_j<1$, but the extended range for $K_j$ simplifies
the calculations a lot. The VBM equations are
\begin{align}
  n_j=& \tilde n_j+M_j,\label{seq:nj}\\
  c_j=& \tilde c_j+C_j+\tilde v_j\tilde\mu_j^2
  -\frac{\big(\tilde v_j\tilde\mu_j+U_j\big)^2}{\tilde v_j+V_j},\label{seq:cj}\\
  v_j=&\tilde v_j+V_j,\label{seq:vj}\\
  \mu_j=&\frac{\tilde v_j\tilde\mu_j+U_j}{\tilde v_j+V_j},\label{seq:muj}\\
\end{align}
where hyperparameters describing the prior distribution are again
indicated by $\tilde{\phantom{n}}$, and the (conjugate) prior
distributions are recovered by setting $M_j=C_j=U_j=V_j=0$. These
data-dependent terms are given by
\begin{align}
  M_j=&\sum_{t=2}^T\mean{\delta_{s_t,j}},\label{seq:Mj}\\
  C_j=&\sum_{t=2}^T\mean{\delta_{s_t,j}}\x_t^2,\label{seq:Cj}\\
  V_j=&\sum_{t=2}^T\mean{\delta_{s_t,j}}\x_{t-1}^2.\label{seq:Vj}\\
  U_j=&\sum_{t=2}^T\mean{\delta_{s_t,j}}\x_t\cdot\x_{t-1}.\label{seq:Uj}
\end{align}

Some useful expectation values for future reference are
\begin{align}
  \mean{K_j}_{q(\vec{B},\vec{K})}=&\mu_j,  \label{seq:meanK}\\
  \text{Var}[K_j]_{q(\vec{B},\vec{K})}=&\frac{c_j}{2v_j(n_j-\frac 12)}.
  \label{seq:varK}\\
  \mean{B_j}_{q(\vec{B},\vec{K})}=&\frac{n_j+\frac 12}{c_j},  \label{seq:meanB}\\
  \text{Var}[B_j]_{q(\vec{B},\vec{K})}=&\frac{n_j+\frac 12}{c_j^2},  
  \label{seq:varB}\\
  \mean{\ln B_j}_{q(\vec{B},\vec{K})}=&\psi\big(n_j+\frac 12\big)-\ln c_j,
  \label{seq:meanlogB}\\
  \mean{B_jK_j^2}_{q(\vec{B},\vec{K})}=&\frac{1}{2v_j}
  +\mu_j^2\frac{n_j+\frac 12}{c_j},  \label{seq:meanBK2}\\
  \mean{B_jK_j}_{q(\vec{B},\vec{K})}=&\mu_j\frac{n_j+\frac 12}{c_j},
  \label{seq:meanBK}
\end{align}

In addition to being needed during the algorithm, these averages can
be used to translate prior knowledge in terms of means and standard
deviations of $K_j,B_j$ into prior parameters $\tilde n_j, \tilde c_j,
\tilde \mu_j, \tilde v_j$

\subsubsection{Hidden state distribution}
The variational distribution has a simple form,
\begin{equation}\label{seq:qgt}
  \ln q(s_{1:T})=-\ln Z+\sum_{t=1}^T\ln h_{s_t}(t)+\sum_{t=2}^T\ln J_{s_{t-1},s_t},
\end{equation}
i.e., an initial state distribution, a point-wise term that depends on
the initial conditions and the data, and a transition
probability. The point-wise 
\begin{equation}
  \ln q(s_{1:T})=-\ln Z+\sum_{t=1}^T\ln h_{s_t}(t)+\sum_{t=2}^T\ln J_{s_{t-1},s_t},
\end{equation}
i.e., an initial state distribution, point-wise terms that depends on
the initial conditions and the data, and transition terms. The
mathematical form of this expression is the same as encountered in
maximum-likelihood optimization of hidden Markov Models, and hence the
normalization constant and expectation values needed for the parameter
update equations can be computed by the Baum-Welch algorithm
\cite{baum1970}, which resembles the transfer matrix solution for spin
models in statistical physics. 

Similarly, and the most likely sequence of hidden states can be
computed by the Viterbi algorithm \cite{viterbi1967}.

Specifically, the initial term is given by
\begin{equation}
  \ln h_j(1)=
  \mean{p(s_1=j|\vec{\pi})}_{q(\vec{\pi})}
  =\psi(w_{j}^{(\vec{\pi})})-\psi(w_0^{(\vec{\pi})}),
\end{equation}
the point-wise contributions for $t>1$ are
\begin{multline}
  \ln h_j(t)=
  \psi\big(n_j+\frac12\big)-\ln(\pi c_j)-\frac{\x_{t-1}^2}{2v_j}\\
-\frac{n_j+\frac12}{c_j}\Bigg(
\x_{t-1}^2\bigg(\mu_j-\frac{\x_t\cdot\x_{t-1}}{\x_{t-1}^2}\bigg)^2\\
+\x_t^2-\frac{\big(\x_t\cdot\x_{t-1}\big)^2}{\x_{t-1}^2}\Bigg),
\end{multline}
and the transition terms are given by
\begin{equation}
    \ln J_{ji}=\psi\big(w_{j,i}^{(\matris{A})}\big)
    -\psi\big(\sum_{k=1}^Nw_{j,k}^{(\matris{A})}\big).
\end{equation}
\subsection{VBEM iterations and model search}
The iterative optimization of the variational distributions are done
as follows. To start with, an initial guess for the variational
parameter distributions are generated. We then alternate between VBE
step, in which we construct the hidden state distribution and compute
the averages $\mean{\delta_{j,s_t}}_{q(s)}$ and
$\mean{\delta_{j,s_t}\delta_{k,s_{t+1}}}_{q(s)}$ in a Baum-Welch
forward-backward sweep, and a VBM step, in which we use these averages
to update the parameter variational distributions, until the lower
bound converges.

The variational approach has the additional useful tendency to
penalizing overfitting already during the VBEM iterations, by
depopulating superfluous states
\cite{mackay1997,beal2003,persson2013}. We exploit this property by
using a greedy search algorithm to explore the model space. The basic
strategy is to start by fitting a model with many states from random
initial conditions, and then exploring less complex models by
gradually removing the least populated states. This saves computing
time by supplying good initial guesses for the low complexity models
(which therefore converge quickly), and by lowering the number of
independent restarts, since it is easier to construct a good initial
guess for a model with many states.

\subsection{The lower bound}
The lower bound has an especially simple form just after the VBE step
\cite{mackay1997,beal2003,persson2013}, given by the normalization
constant $\ln Z$ of the variational hidden state distribution, minus
the Kullback-Leibler divergences between the variational and prior
parameter distributions,
\begin{multline}
  F=\ln Z
  -\int d\vec{\pi} q(\vec{\pi}) \ln\frac{q(\vec{\pi})}{p(\vec{\pi})}\\
  -\sum_{j=1}^N\Bigg[
  \int d^NA_{j,:}\;q(A_{j,:})\ln\frac{q(A_{j,:})}{p_0(A_{j,:})}\\
  +\int dB_jdK_j\;q(B_j,K_j)\ln\frac{q(B_j,K_j)}{p_0(B_j,K_j)}
  \Bigg].
\end{multline}
The Kullback-Leibler terms can be expressed in terms of the
expectation values computed above. For the initial state distribution,
we get
\begin{multline}
  \int d\vec{\pi} q(\vec{\pi}) \ln\frac{q(\vec{\pi})}{p_0(\vec{\pi})}
  =\ln\tilde w_0^{(\vec{\pi})}
    -\psi(\tilde w_0^{(\vec{\pi})})-\frac{1}{\tilde w_0^{(\vec{\pi})}}\\
    +\sum_{j=1}^N\left[
      \big(w_j^{(\vec{\pi})}-\tilde w_j^{(\vec{\pi})}\big)\psi(w_j^{(\vec{\pi})})
      -\ln\frac{\Gamma(w_j^{(\vec{\pi})})}{\Gamma(\tilde w_j^{(\vec{\pi})})}
      \right].
\end{multline}
To get this simple form, we used that $w_0^{(\vec{\pi})}=1+\tilde
w_0^{(\vec{\pi})}$ (since $\sum_j\mean{\delta_{j,s_1}}=1$), and the
identities $\Gamma(x+1)=x\Gamma(x)$ and $\psi(x+1)=\psi(x)+\frac 1x$.
Furthermore, each row of the transition probability matrix contributes
\begin{multline}
  \int d^NA_{j,:}\;q(A_{j,:})\ln\frac{q(A_{j,:})}{p_0(A_{j,:})}\\
  =\ln\frac{\Gamma(w_{j0}^{(\matris{A})})}{\Gamma(\tilde w_{j0}^{(\matris{A})})}
    -(w_{j0}^{(\matris{A})}-\tilde w_{j0}^{(\matris{A})})\psi(w_{j0}^{(\matris{A})})\\
    -\sum_{k=1}^N\bigg[
      \ln\frac{\Gamma(w_{jk}^{(\matris{A})})}{\Gamma(\tilde w_{jk}^{(\matris{A})})}
      -\big(w_{jk}^{(\matris{A})}-\tilde w_{jk}^{(\matris{A})}\big)
      \psi(w_{jk}^{(\matris{A})})\bigg].
\end{multline}
Finally, the emission parameter of each state contributes
\begin{multline}\label{seq:KBKL}
  \int dB_j\int d^NK_j\;q(B_j,K_j)
  \ln\frac{q(B_j,K_j)}{p(B_j,K_j)}=\ldots\\
  =-\frac{n_j+\frac 12}{c_j}
    \Big(c_j-\tilde c_j-\tilde v_j(\mu_j-\tilde \mu_j)^2\Big)\\
    +\frac 12\ln\frac{v_j}{\tilde v_j}
    +(\tilde n_j+\frac 12)\ln\frac{c_j}{\tilde c_j}
    -\ln\frac{\Gamma\big(n_j+\frac 12\big)}{\Gamma\big(\tilde n_j+\frac 12\big)}\\
    +(n_j-\tilde n_j)\psi\big(n_j+\frac 12\big)
    +\frac{\tilde v_j}{2v_j}
    -\frac 12.
\end{multline}


\subsection{Extension to a factorial model}
The above algorithm is readily extended to treat the factorial model
where genuine and spurious states are separated into two different
hidden processes. The extra process is called , $c_t$, where $c_t=1$
indicates genuine looping dynamics goverened by the simple model
described above. However, when $c_t>1$, the bead motion is modeled by
a different set of emission parameters $\hat B_{c_t},\hat
K_{c_t}$. This separates experimental artifacts from genuine looping
dynamics. We assume the genuine states, $s_t$, to evolve independently
of spurious events. Similarly, spurious events $c_t>1$ can
interconvert independently of the underlying genuine state, but we
take transitions out of $c_t=1$ to depend on $s_t$, to allow for,
e.g., the possibility that transient sticking events are more frequent
in a looped state when the bead is on average closer to the
coverslip. \cite{notfinished}

%%%

\begin{equation}
  p(s_{t+1},c_{t+1}|s_t,c_t)=p(s_{t+1}|s_t)p(c_{t+1}|s_t,c_t),
\end{equation}
with $p(s_{t+1}|s_t)=A_{s_ts_{t+1}}$,
\begin{equation}
  p(c_{t+1}|s_t,c_t)
  =\left\{
  \begin{array}{ll}
    \hat A_{s_tc_{t+1}},&\text{ if $c_t=1$,}\\
    \hat R_{c_tc_{t+1}},&\text{ if $c_t>1$,}\\
  \end{array}
  \right.
\end{equation}


This has a significant computational cost, since a simple model with
$N_{gen.}$ genuine states and $N_{sp.}$ spurious ones gets
$N_{gen.}\times(1+N_{sp.})$ states after conversion. However, since we
do not perform exhaustive model search in this representation and can
utilize the simpler model to make good initial guesses, this is not a
significant problem.


{\bf Prior for factorial model: TBA}.\cite{factorialmodelprior}


\subsection{Specification of prior distributions}
We specify priors for the emission parameters $K,B$ in terms of the
the mean values $\mean{K_j}$, $\mean{B_j}$, the standard deviation of
$K_j$, and the number of pseudo-counts $\tilde n_j$, and then solve
for the remaining hyper-parameters $\tilde c_j, \tilde \mu_j,\tilde
v_j$ using Eqs.~(\ref{seq:meanK}-\ref{seq:varB}).  We take the same
hyperparameters for all states, and independent of the number of
states $N$. Priors for spurious states are specified analogously.

The initial state probability vector has a Dirichlet prior with
weights $\tilde\vec w^{(\vec{\pi})}$ (see \Eq{seq:VBMpi}). We choose a
constant total strength $f^{(\vec\pi)}$, i.e.,
\begin{equation}
  \tilde w_j^{(\vec{\pi})}=f^{(\vec\pi)}/N.
\end{equation}

The prior for the transition matrix is independent Dirichlet
distributions for each row (see \Eq{seq:VBMAj}), with pseudo-count
matrix $\tilde w_{ij}^{(\matris{A})}$. Following \citet{persson2013},
we parameterize this prior in terms of an expected mean dwell time and
an overall number of pseudocounts (prior strength) for each hidden
state. To make the definition invariant under changes of sampling
time, we specify the strength $t_A$ and prior mean dwell time $t_D$ in
time units. We then construct a transition \textit{rate} matrix Q with
mean dwell time $t_D$,
\begin{equation}
  Q_{ij}=\frac{1}{t_D}\left(-\delta_{ij}
  +\frac{1-\delta_{ij}}{N-1}\right),
\end{equation}
and construct the pseudo-counts based on the transition probability
propagator per unit time step,
\begin{equation}
  \tilde w_{ij}^{(\matris{A})}=\frac{t_Af_{sample}}{n_{downsample}}
  e^{\Delta tQ}.
\end{equation}
This expression uses the downsampled timestep $\Delta
t=n_{downsample}/f_{sample}$, where $f_{sample}$ is the sampling
frequency (30 Hz in our case), and $n_{downsample}$ is the
downsampling factor.  Numerical experiments by \citet{persson2013}
show that choosing the strength too low compared to the mean dwell
time produces a bias towards sparse transition matrices. This is not
desireable in our case, and we therefore use $t_D=1$ s, and $t_A=5$ s
throughout this work.

Runinput parameters for prior specification is given in table
\ref{stab:priors}.

\subsection{Empirical Bayes update equations}
The empirical Bayes update equations optimizes the lower bound with
respect to the hyperparameters in the prior distribution. This means
optimizing sums of Kullback-Leibler divergence terms. 

The initial state probability, and the rows of the transition
probability matrix, are both Dirichlet distributed. Thus, for $M$
trajectories with Dirichlet parameters $u_j^{(i)}$, $i=1,2,\ldots,M$,
and hyperparameters $\tilde{u}_j$ ($u = w^{(\vec\pi)},u^{(\matris{A})}$),
we need to solve
\begin{multline}
\frac{d}{d \tilde u_j} \sum_i\Bigg( 
 \ln\frac{\Gamma(u_0^{(i)})}{\Gamma(\tilde u_0)}
    -(u_0^{(i)}-\tilde u_0)\psi(u_0^{(i)})\\
    -\sum_{k=1}^N\bigg[
      \ln\frac{\Gamma(u_k^{(i)})}{\Gamma(\tilde u_k^{(i)})}
      -\big(u_k^{(i)}-\tilde u_k\big)
      \psi(u_k^{(i)})\bigg]\Bigg)=0,
\end{multline}
where $u_0^{(i)}=\sum_ku_k^{(i)}$ and similar for $\tilde u_0$. This
leads to the update equations
\begin{equation}
  \psi(\tilde u_0)-\psi(\tilde u_j)
=\frac{1}{M}\sum_i\bigg(
  \psi(u_0^{(i)})-\psi(u_j^{(i)})\bigg).
\end{equation}
A numerical solution turned out to be easier using the variables
$\tilde U_j=\ln \tilde u_j$ (to numerically enforce $\tilde u_j>0$).

For the emission parameters, the update equations are instead derived
from minimizing \Eq{seq:KBKL} summed over $M$ trajectories,
\begin{multline}
  f_{KB}=\sum_i\Bigg(
  -\frac{n^{(i)}+\frac 12}{c^{(i)}} \Big(c^{(i)}-\tilde c-\tilde v(\mu^{(i)}-\tilde
  \mu)^2\Big)\\ +\frac 12\ln\frac{v^{(i)}}{\tilde v} +(\tilde
  n+\frac 12)\ln\frac{c^{(i)}}{\tilde c}
  -\ln\frac{\Gamma\big(n^{(i)}+\frac 12\big)}{\Gamma\big(\tilde n+\frac
    12\big)}\\ +(n^{(i)}-\tilde n)\psi\big(n^{(i)}+\frac 12\big)
  +\frac{1}{2}\Big(\frac{\tilde v}{v^{(i)}} -1\Big)\Bigg).
\end{multline}
Minimizing with respect to $\tilde\mu$ and $\tilde v$ leads to
\begin{align}
   \tilde\mu=&\frac{1}{M}\sum_i\mu^{(i)},\\
   \frac{1}{\tilde v}=&
   \frac{1}{M}\sum_i\Big(\frac{1}{v^{(i)}}+2(\tilde\mu-\mu^{(i)})^2\Big).
\end{align}
The remaining $\tilde c$ and $\tilde n$ lead to
\begin{align}
  \frac{\tilde n+\frac 12}{\tilde c}=&
  \frac{1}{M}\sum_i\frac{n^{(i)}+\frac 12}{c^{(i)}},\\
  \ln\tilde c-\psi\big(\tilde n+\frac 12\big)=&
  \frac{1}{M}\sum_i\bigg(
\ln c^{(i)}-\psi\big(n^{(i)}+\frac 12\big)\bigg),
\end{align}
which we solve numerically. This gets easier by defining
$\alpha=\frac{\tilde n+\frac 12}{\tilde c}$, then solve the second
equation for $\tilde c$ numerically, and finally compute $\tilde
n=\alpha\tilde c-\frac 12$.
