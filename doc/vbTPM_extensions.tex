\subsection{Empirical Bayes update equations}
The empirical Bayes update equations optimizes the lower bound with
respect to the hyperparameters in the prior distribution. This means
optimizing sums of Kullback-Leibler divergence terms. 

The initial state probability, and the rows of the transition
probability matrix, are both Dirichlet distributed. Thus, for $M$
trajectories with Dirichlet parameters $u_j^{(i)}$, $i=1,2,\ldots,M$,
and hyperparameters $\tilde{u}_j$ ($u = w^{(\vec\pi)},u^{(\matris{A})}$),
we need to solve
\begin{multline}
\frac{d}{d \tilde u_j} \sum_i\Bigg( 
 \ln\frac{\Gamma(u_0^{(i)})}{\Gamma(\tilde u_0)}
    -(u_0^{(i)}-\tilde u_0)\psi(u_0^{(i)})\\
    -\sum_{k=1}^N\bigg[
      \ln\frac{\Gamma(u_k^{(i)})}{\Gamma(\tilde u_k^{(i)})}
      -\big(u_k^{(i)}-\tilde u_k\big)
      \psi(u_k^{(i)})\bigg]\Bigg)=0,
\end{multline}
where $u_0^{(i)}=\sum_ku_k^{(i)}$ and similar for $\tilde u_0$. This
leads to the update equations
\begin{equation}
  \psi(\tilde u_0)-\psi(\tilde u_j)
=\frac{1}{M}\sum_i\bigg(
  \psi(u_0^{(i)})-\psi(u_j^{(i)})\bigg).
\end{equation}
A numerical solution turned out to be easier using the variables
$\tilde U_j=\ln \tilde u_j$ (to numerically enforce $\tilde u_j>0$).

For the emission parameters, the update equations are instead derived
from minimizing \Eq{seq:KBKL} summed over $M$ trajectories,
\begin{multline}
  f_{KB}=\sum_i\Bigg(
  -\frac{n^{(i)}+\frac 12}{c^{(i)}} \Big(c^{(i)}-\tilde c-\tilde v(\mu^{(i)}-\tilde
  \mu)^2\Big)\\ +\frac 12\ln\frac{v^{(i)}}{\tilde v} +(\tilde
  n+\frac 12)\ln\frac{c^{(i)}}{\tilde c}
  -\ln\frac{\Gamma\big(n^{(i)}+\frac 12\big)}{\Gamma\big(\tilde n+\frac
    12\big)}\\ +(n^{(i)}-\tilde n)\psi\big(n^{(i)}+\frac 12\big)
  +\frac{1}{2}\Big(\frac{\tilde v}{v^{(i)}} -1\Big)\Bigg).
\end{multline}
Minimizing with respect to $\tilde\mu$ and $\tilde v$ leads to
\begin{align}
   \tilde\mu=&\frac{1}{M}\sum_i\mu^{(i)},\\
   \frac{1}{\tilde v}=&
   \frac{1}{M}\sum_i\Big(\frac{1}{v^{(i)}}+2(\tilde\mu-\mu^{(i)})^2\Big).
\end{align}
The remaining $\tilde c$ and $\tilde n$ lead to
\begin{align}
  \frac{\tilde n+\frac 12}{\tilde c}=&
  \frac{1}{M}\sum_i\frac{n^{(i)}+\frac 12}{c^{(i)}},\\
  \ln\tilde c-\psi\big(\tilde n+\frac 12\big)=&
  \frac{1}{M}\sum_i\bigg(
\ln c^{(i)}-\psi\big(n^{(i)}+\frac 12\big)\bigg),
\end{align}
which we solve numerically. This gets easier by defining
$\alpha=\frac{\tilde n+\frac 12}{\tilde c}$, then solve the second
equation for $\tilde c$ numerically, and finally compute $\tilde
n=\alpha\tilde c-\frac 12$.

\subsection{Doing empirical Bayes}
Our empirical Bayes analysis of multiple models was not implemented as
part of our analysis pipeline, but instead ran using tailored
scripts. These are not included with vbTPM, but the optimization
procedures for the individual prior distributions are included, in
\verb+VB7_EBupdate_dirichlet+, and \verb+VB7_EBupdate_KB+.
